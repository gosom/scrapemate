package scrapemate

import (
	"context"
	"errors"
	"fmt"
	"os"
	"os/signal"
	"runtime/debug"
	"sync"
	"syscall"
	"time"

	"github.com/gosom/kit/logging"
)

// New creates a new scrapemate
func New(options ...func(*ScrapeMate) error) (*ScrapeMate, error) {
	mu := &sync.Mutex{}
	s := &ScrapeMate{
		lock:        &sync.RWMutex{},
		refreshCond: sync.NewCond(mu),
		refreshing:  false,
	}

	for _, opt := range options {
		if err := opt(s); err != nil {
			return nil, err
		}
	}

	if s.jobProvider == nil {
		return nil, ErrorNoJobProvider
	}

	if s.httpFetcher == nil {
		return nil, ErrorNoHTMLFetcher
	}
	// here we can set default options
	s.results = make(chan Result)

	if s.ctx == nil {
		s.ctx, s.cancelFn = context.WithCancelCause(context.Background())
	}

	if s.cancelFn == nil {
		s.ctx, s.cancelFn = context.WithCancelCause(s.ctx)
	}

	if s.log == nil {
		s.log = logging.Get().With("component", "scrapemate")
		s.log.Debug("using default logger")
	}

	if s.concurrency == 0 {
		s.concurrency = 1
	}

	return s, nil
}

func WithExitBecauseOfInactivity(duration time.Duration) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		s.exitOnInactivity = duration > 0
		s.exitOnInactivityDuration = duration

		return nil
	}
}

// WithFailed sets the failed jobs channel for the scrapemate
func WithFailed() func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		s.failedJobs = make(chan IJob)
		return nil
	}
}

// WithContext sets the context for the scrapemate
func WithContext(ctx context.Context, cancelFn context.CancelCauseFunc) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if ctx == nil {
			return ErrorNoContext
		}

		s.ctx = ctx
		s.cancelFn = cancelFn

		return nil
	}
}

// WithLogger sets the logger for the scrapemate
func WithLogger(log logging.Logger) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if log == nil {
			return ErrorNoLogger
		}

		s.log = log

		return nil
	}
}

// WithJobProvider sets the job provider for the scrapemate
func WithJobProvider(provider JobProvider) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if provider == nil {
			return errors.New("job provider is nil")
		}

		s.jobProvider = provider

		return nil
	}
}

// WithConcurrency sets the concurrency for the scrapemate
func WithConcurrency(concurrency int) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if concurrency < 1 {
			return ErrorConcurrency
		}

		s.concurrency = concurrency

		return nil
	}
}

// WithHTTPFetcher sets the http fetcher for the scrapemate
func WithHTTPFetcher(client HTTPFetcher) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if client == nil {
			return ErrorNoHTMLFetcher
		}

		s.httpFetcher = client

		return nil
	}
}

func WithAlternativeHTTPFetcher(client HTTPFetcher) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if client == nil {
			return ErrorNoHTMLFetcher
		}

		s.alternativeHTTPFetcher = client

		return nil
	}
}

// WithHTMLParser sets the html parser for the scrapemate
func WithHTMLParser(parser HTMLParser) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if parser == nil {
			return ErrorNoHTMLParser
		}

		s.htmlParser = parser

		return nil
	}
}

// WithCache sets the cache for the scrapemate
func WithCache(cache Cacher) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if cache == nil {
			return ErrorNoCacher
		}

		s.cache = cache

		return nil
	}
}

// WithInitJob sets the first job to be processed
// It will be processed before the jobs from the job provider
// It is useful if you want to start the scraper with a specific job
// instead of the first one from the job provider
// A real use case is when you want to obtain some cookies before starting
// the scraping process (e.g. login)
// Important: The results from these job will be discarded !
func WithInitJob(job IJob) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		s.initJob = job

		return nil
	}
}

func WithInternetProvider(provider InternetProvider) func(*ScrapeMate) error {
	return func(s *ScrapeMate) error {
		if provider == nil {
			return ErrorNoInternetProvider
		}

		s.internetProvider = provider

		return nil
	}
}

// Scrapemate contains unexporter fields
type ScrapeMate struct {
	log                    logging.Logger
	ctx                    context.Context
	cancelFn               context.CancelCauseFunc
	jobProvider            JobProvider
	concurrency            int
	httpFetcher            HTTPFetcher
	alternativeHTTPFetcher HTTPFetcher
	htmlParser             HTMLParser
	cache                  Cacher
	results                chan Result
	failedJobs             chan IJob
	initJob                IJob

	stats                    stats
	exitOnInactivity         bool
	exitOnInactivityDuration time.Duration

	lock             *sync.RWMutex
	refreshCond      *sync.Cond
	refreshing       bool
	activeReqs       int
	currentGwVersion int64
	internetProvider InternetProvider
}

// Start starts the scraper
func (s *ScrapeMate) Start() error {
	s.log.Info("starting scrapemate")

	defer func() {
		close(s.results)

		if s.failedJobs != nil {
			close(s.failedJobs)
		}

		s.log.Info("scrapemate exited")
	}()

	exitChan := make(chan os.Signal, 1)

	signal.Notify(exitChan, os.Interrupt, syscall.SIGTERM)
	s.waitForSignal(exitChan)

	if s.internetProvider != nil {
		if err := s.refreshIP(s.ctx, s.currentGwVersion); err != nil {
			return err
		}
	}

	if err := s.processInitJob(s.ctx); err != nil {
		return err
	}

	wg := sync.WaitGroup{}
	wg.Add(s.concurrency)

	for i := 0; i < s.concurrency; i++ {
		go func() {
			defer wg.Done()

			s.startWorker(s.ctx)
		}()
	}

	wg.Add(1)

	go func() {
		defer wg.Done()

		startTime := time.Now().UTC()
		tickerDur := time.Minute

		const (
			divider          = 2
			secondsPerMinute = 60
		)

		if s.exitOnInactivity && s.exitOnInactivityDuration < tickerDur {
			tickerDur = s.exitOnInactivityDuration / divider
		}

		ticker := time.NewTicker(tickerDur)
		defer ticker.Stop()

		for {
			select {
			case <-s.ctx.Done():
				return
			case <-ticker.C:
				numOfJobsCompleted, numOfJobsFailed, lastActivityAt := s.stats.getStats()
				perMinute := float64(numOfJobsCompleted) / time.Now().UTC().Sub(startTime).Seconds() * secondsPerMinute

				s.log.Info("scrapemate stats",
					"numOfJobsCompleted", numOfJobsCompleted,
					"numOfJobsFailed", numOfJobsFailed,
					"lastActivityAt", lastActivityAt,
					"speed", fmt.Sprintf("%.2f jobs/min", perMinute),
				)

				if s.exitOnInactivity && time.Now().UTC().Sub(lastActivityAt) > s.exitOnInactivityDuration {
					err := fmt.Errorf("%w: %s", ErrInactivityTimeout, lastActivityAt.Format(time.RFC3339))

					s.log.Info("exiting because of inactivity", "error", err)
					s.cancelFn(err)

					return
				}
			}
		}
	}()

	wg.Wait()

	<-s.Done()

	return s.Err()
}

// Concurrency returns how many workers are running in parallel
func (s *ScrapeMate) Concurrency() int {
	return s.concurrency
}

// Results returns a channel containing the results
func (s *ScrapeMate) Results() <-chan Result {
	return s.results
}

// Failed returns the chanell that contains the jobs that failed. It's nil if
// you don't use the WithFailed option
func (s *ScrapeMate) Failed() <-chan IJob {
	return s.failedJobs
}

// DoJob scrapes a job and returns it's result
func (s *ScrapeMate) DoJob(ctx context.Context, job IJob) (result any, next []IJob, err error) {
	ctx = ContextWithLogger(ctx, s.log.With("jobid", job.GetID()))
	startTime := time.Now().UTC()

	s.log.Debug("starting job", "job", job)

	var resp Response

	defer func() {
		args := []any{
			"job", job,
		}

		if r := recover(); r != nil {
			args = append(args, "error", r, "status", "failed")
			stack := string(debug.Stack())
			err = fmt.Errorf("panic while executing job: %s", stack)
			args = append(args, "error", err)
			s.log.Error("job finished", args...)

			return
		}

		if resp.Error != nil {
			args = append(args, "error", resp.Error, "status", "failed")
		} else {
			args = append(args, "status", "success")
		}

		args = append(args, "duration", time.Now().UTC().Sub(startTime))

		s.log.Info("job finished", args...)
	}()

	var cached bool

	cacheKey := job.GetCacheKey()

	if s.cache != nil {
		var errCache error

		resp, errCache = s.cache.Get(ctx, cacheKey)
		if errCache == nil {
			cached = true
		}
	}

	switch {
	case cached:
		s.log.Debug("using cached response", "job", job)
	default:
		resp = s.doFetch(ctx, job)
		if !job.ProcessOnFetchError() && resp.Error != nil {
			err = resp.Error

			return nil, nil, err
		}

		// check if resp.Error is valid because we may ProcessOnFetchError
		if resp.Error == nil && s.cache != nil {
			if errCache := s.cache.Set(ctx, cacheKey, &resp); errCache != nil {
				s.log.Error("error while caching response", "error", errCache, "job", job)
			}
		}
	}

	// process the response if we have a html parser and the resp has no error
	if resp.Error == nil && s.htmlParser != nil {
		resp.Document, err = s.htmlParser.Parse(ctx, resp.Body)
		if err != nil {
			s.log.Error("error while setting document", "error", err)

			return nil, nil, err
		}
	}

	result, next, err = job.Process(ctx, &resp)
	if err != nil {
		// TODO shall I retry?
		s.log.Error("error while processing job", "error", err)

		return nil, nil, err
	}

	return result, next, nil
}

func (s *ScrapeMate) doFetch(ctx context.Context, job IJob) (ans Response) {
	var ok bool
	defer func() {
		if !ok && ans.Error == nil {
			ans.Error = fmt.Errorf("status code %d", ans.StatusCode)
		}
	}()

	maxRetries := s.getMaxRetries(job)

	const defaultMilliseconds = 100

	delay := time.Millisecond * defaultMilliseconds
	retryPolicy := job.GetRetryPolicy()
	retry := 0

	for {
		// Wait if a refresh is in progress
		s.refreshCond.L.Lock()
		for s.refreshing {
			s.refreshCond.Wait()
		}

		s.activeReqs++
		currentVersion := s.currentGwVersion
		s.refreshCond.L.Unlock()

		if job.UserAlternativeFetcher() {
			ans = s.alternativeHTTPFetcher.Fetch(ctx, job)
		} else {
			ans = s.httpFetcher.Fetch(ctx, job)
		}

		s.refreshCond.L.Lock()
		s.activeReqs--

		if s.activeReqs == 0 {
			s.refreshCond.Broadcast() // Notify if there are no active requests
		}
		s.refreshCond.L.Unlock()

		ok = job.DoCheckResponse(&ans)

		if ok {
			return ans
		}

		if retryPolicy == DiscardJob {
			s.log.Warn("discarding job because of policy")

			return ans
		}

		if retryPolicy == StopScraping {
			s.log.Warn("stopping scraping because of policy")
			s.cancelFn(errors.New("stopping scraping because of policy"))

			return ans
		}

		if retry >= maxRetries {
			return ans
		}

		retry++

		delayFn := func() {
			time.Sleep(delay)

			if delay > job.GetMaxRetryDelay() {
				delay = job.GetMaxRetryDelay()
			} else {
				delay *= 2
			}
		}

		switch retryPolicy {
		case RetryJob:
			delayFn()
		case RefreshIP:
			s.log.Warn("refreshing ip because of policy", "url", job.GetURL(), "retry", retry, "maxRetries", maxRetries, "delay", delay, "version", currentVersion)

			if err := s.refreshIP(ctx, currentVersion); err != nil {
				s.log.Error("error while refreshing ip", "error", err)

				return ans
			}
		}
	}
}

func (s *ScrapeMate) refreshIP(ctx context.Context, version int64) error {
	s.refreshCond.L.Lock()
	defer func() {
		s.refreshing = false
		s.refreshCond.Broadcast()
		s.refreshCond.L.Unlock()
	}()

	s.refreshing = true

	// Wait until there are no active requests
	for s.activeReqs > 0 {
		s.refreshCond.Wait()
	}

	if version != s.currentGwVersion {
		s.log.Warn("gateway version changed, not refreshing ip")

		return nil
	}

	currentGwVersion := s.currentGwVersion + 1

	t0 := time.Now().UTC()

	if s.internetProvider == nil {
		s.log.Warn("no internet provider set")
	} else {
		err := s.internetProvider.RenewIP(ctx)
		if err != nil {
			return err
		}
	}

	s.currentGwVersion = currentGwVersion

	s.log.Info("ip refreshed", "duration", time.Now().UTC().Sub(t0))

	return nil
}

func (s *ScrapeMate) getMaxRetries(job IJob) int {
	const maxRetriesDefault = 5

	maxRetries := job.GetMaxRetries()
	if maxRetries > maxRetriesDefault {
		maxRetries = maxRetriesDefault
	}

	return maxRetries
}

// Done returns a channel  that's closed when the work is done
func (s *ScrapeMate) Done() <-chan struct{} {
	return s.ctx.Done()
}

// Err returns the error that caused scrapemate's context cancellation
func (s *ScrapeMate) Err() error {
	err := context.Cause(s.ctx)
	if errors.Is(err, ErrInactivityTimeout) {
		return nil
	}

	return err
}

func (s *ScrapeMate) waitForSignal(sigChan <-chan os.Signal) {
	go func() {
		<-sigChan
		s.log.Info("received signal, shutting down")
		s.cancelFn(ErrorExitSignal)
	}()
}

func (s *ScrapeMate) processInitJob(ctx context.Context) error {
	if s.initJob == nil {
		return nil
	}

	s.log.Info("processing init", "job", s.initJob)
	defer s.log.Info("init job finished", "job", s.initJob)

	var stack []IJob

	if s.initJob != nil {
		stack = append(stack, s.initJob)
	}

	var job IJob

	for len(stack) > 0 {
		select {
		case <-ctx.Done():
			return nil
		default:
		}

		job, stack = stack[0], stack[1:]

		_, next, err := s.DoJob(ctx, job)
		if err != nil {
			return err
		}

		stack = append(stack, next...)
	}

	return nil
}

func (s *ScrapeMate) startWorker(ctx context.Context) {
	jobc, errc := s.jobProvider.Jobs(ctx)

	for {
		select {
		case <-ctx.Done():
			return
		case err := <-errc:
			if ctx.Err() == context.Canceled {
				return
			}

			s.log.Error("error while getting jobs...going to wait a bit", "error", err)

			time.Sleep(1 * time.Second)

			jobc, errc = s.jobProvider.Jobs(ctx)

			s.log.Info("restarted job provider")
		case job := <-jobc:
			ans, next, err := s.DoJob(ctx, job)
			if err != nil {
				s.log.Error("error while processing job", "error", err)

				s.pushToFailedJobs(job)

				continue
			}

			if err := s.finishJob(ctx, job, ans, next); err != nil {
				s.log.Error("error while finishing job", "error", err)

				s.pushToFailedJobs(job)
			}
		}
	}
}

func (s *ScrapeMate) pushToFailedJobs(job IJob) {
	s.stats.incJobsFailed()

	if s.failedJobs != nil {
		s.failedJobs <- job
	}
}

func (s *ScrapeMate) finishJob(ctx context.Context, job IJob, ans any, next []IJob) error {
	s.stats.incJobsCompleted()

	if err := s.pushJobs(ctx, next); err != nil {
		return fmt.Errorf("%w: while pushing jobs", err)
	}

	if job.UseInResults() {
		s.results <- Result{
			Job:  job,
			Data: ans,
		}
	}

	return nil
}

func (s *ScrapeMate) pushJobs(ctx context.Context, jobs []IJob) error {
	for i := range jobs {
		if err := s.jobProvider.Push(ctx, jobs[i]); err != nil {
			return err
		}
	}

	return nil
}

type stats struct {
	l                  sync.RWMutex
	numOfJobsCompleted int64
	numOfJobsFailed    int64
	lastActivityAt     time.Time
}

func (o *stats) getStats() (completed, failed int64, lastActivityAt time.Time) {
	o.l.RLock()
	defer o.l.RUnlock()

	return o.numOfJobsCompleted, o.numOfJobsFailed, o.lastActivityAt
}

func (o *stats) incJobsCompleted() {
	o.l.Lock()
	defer o.l.Unlock()

	o.numOfJobsCompleted++
	o.lastActivityAt = time.Now().UTC()
}

func (o *stats) incJobsFailed() {
	o.l.Lock()
	defer o.l.Unlock()

	o.numOfJobsFailed++
	o.lastActivityAt = time.Now().UTC()
}
